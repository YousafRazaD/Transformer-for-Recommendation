{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8153291,"sourceType":"datasetVersion","datasetId":4822445},{"sourceId":8154524,"sourceType":"datasetVersion","datasetId":4823393},{"sourceId":8159218,"sourceType":"datasetVersion","datasetId":4827023}],"dockerImageVersionId":30683,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nimport pandas as pd\nimport numpy as np\nimport random","metadata":{"id":"HYXUsIQ14ptb","execution":{"iopub.status.busy":"2024-04-19T11:11:37.202461Z","iopub.execute_input":"2024-04-19T11:11:37.203249Z","iopub.status.idle":"2024-04-19T11:11:41.746258Z","shell.execute_reply.started":"2024-04-19T11:11:37.203218Z","shell.execute_reply":"2024-04-19T11:11:41.745318Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"Setting Parameters which are used in the model.\nThese Parameters can be changed.","metadata":{}},{"cell_type":"code","source":"# hyperparameters\nbatch_size = 64 # how many independent sequences will we process in parallel?\nblock_size = 256 # what is the maximum context length for predictions?\nmax_iters = 5000\neval_interval = 100\nlearning_rate = 1e-4\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 100\nn_embd = 576\nn_head = 6\nn_layer = 6\nvocab_size = 3953\ndropout = 0.3\n# ------------","metadata":{"id":"OxA_BBHVHvt7","execution":{"iopub.status.busy":"2024-04-19T11:11:41.748385Z","iopub.execute_input":"2024-04-19T11:11:41.748776Z","iopub.status.idle":"2024-04-19T11:11:41.774003Z","shell.execute_reply.started":"2024-04-19T11:11:41.748751Z","shell.execute_reply":"2024-04-19T11:11:41.773112Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"Importing Dataset.\nThis dataset is a modified version of MovieLens-1M dataset.\nDataset contains sequence of movies watched in order of timestamp for all users.","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/dataset2/ratings1.csv\")\ndata = data.drop(\"user_id\",axis=1)\n\ndata","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"-dZQrlf8H31k","outputId":"ed8421ce-44be-422a-f45d-3ea62e795a30","execution":{"iopub.status.busy":"2024-04-19T11:11:41.774999Z","iopub.execute_input":"2024-04-19T11:11:41.775247Z","iopub.status.idle":"2024-04-19T11:11:41.954876Z","shell.execute_reply.started":"2024-04-19T11:11:41.775226Z","shell.execute_reply":"2024-04-19T11:11:41.953946Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                                           interactions\n0     [3186, 1721, 1022, 1270, 2340, 1836, 3408, 120...\n1     [1198, 1217, 1210, 2717, 1293, 2943, 1225, 119...\n2     [593, 2858, 3534, 1968, 1961, 1431, 1266, 1378...\n3     [1210, 1097, 3468, 3527, 480, 260, 1196, 1198,...\n4     [2717, 919, 908, 356, 1250, 2188, 2858, 1127, ...\n...                                                 ...\n6035  [1721, 2376, 3438, 2428, 1883, 2492, 2827, 268...\n6036  [1882, 2028, 1267, 702, 3508, 562, 3148, 858, ...\n6037  [920, 3396, 1210, 2146, 356, 1387, 1079, 1148,...\n6038  [111, 282, 2067, 930, 1230, 3133, 3022, 947, 3...\n6039  [858, 2384, 593, 1961, 2019, 573, 3111, 3505, ...\n\n[6040 rows x 1 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>interactions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[3186, 1721, 1022, 1270, 2340, 1836, 3408, 120...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[1198, 1217, 1210, 2717, 1293, 2943, 1225, 119...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[593, 2858, 3534, 1968, 1961, 1431, 1266, 1378...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[1210, 1097, 3468, 3527, 480, 260, 1196, 1198,...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[2717, 919, 908, 356, 1250, 2188, 2858, 1127, ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>6035</th>\n      <td>[1721, 2376, 3438, 2428, 1883, 2492, 2827, 268...</td>\n    </tr>\n    <tr>\n      <th>6036</th>\n      <td>[1882, 2028, 1267, 702, 3508, 562, 3148, 858, ...</td>\n    </tr>\n    <tr>\n      <th>6037</th>\n      <td>[920, 3396, 1210, 2146, 356, 1387, 1079, 1148,...</td>\n    </tr>\n    <tr>\n      <th>6038</th>\n      <td>[111, 282, 2067, 930, 1230, 3133, 3022, 947, 3...</td>\n    </tr>\n    <tr>\n      <th>6039</th>\n      <td>[858, 2384, 593, 1961, 2019, 573, 3111, 3505, ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>6040 rows Ã— 1 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Splitting Data into Train and Test Sets and converting them into numpy arrays because arrays are currently stored as strings in pandas dataframe which cannot be used in model.\nAll sequences are merged together into one long sequence too.","metadata":{}},{"cell_type":"code","source":"\ntotal_rows = len(data)\nsplit_index = int(0.9 * total_rows)\n\ntrain_data = data.iloc[:split_index]\ntest_data = data.iloc[split_index:]\n\n# Optionally, reset the index for both DataFrames\ntrain_data.reset_index(drop=True, inplace=True)\ntest_data.reset_index(drop=True, inplace=True)\narray1 = train_data['interactions'].apply(eval)\narray2 = test_data['interactions'].apply(eval)\nlong1 = np.concatenate(array1)\nlong2 = np.concatenate(array2)\nprint(len(long1),len(long2))\ntrain_data = long1\ntest_data = long2\n\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"RRh7ArOwKaJP","outputId":"e31d5a9a-248e-46df-e470-8286ee0083e4","execution":{"iopub.status.busy":"2024-04-19T11:11:41.957160Z","iopub.execute_input":"2024-04-19T11:11:41.957484Z","iopub.status.idle":"2024-04-19T11:11:43.451548Z","shell.execute_reply.started":"2024-04-19T11:11:41.957459Z","shell.execute_reply":"2024-04-19T11:11:43.450617Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"899469 100740\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Get Batch function randomly picks sequences of length block_size and the number of sequences is batch_size.\nEach batch is picked from a random position in sequence.","metadata":{}},{"cell_type":"code","source":"def get_batch(split):\n    data = train_data if split == \"train\" else test_data\n    data = torch.tensor(data)\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    x, y = x.to(device), y.to(device)\n    return x, y","metadata":{"id":"tB_t1UDHNaFu","execution":{"iopub.status.busy":"2024-04-19T11:11:43.452716Z","iopub.execute_input":"2024-04-19T11:11:43.453002Z","iopub.status.idle":"2024-04-19T11:11:43.459265Z","shell.execute_reply.started":"2024-04-19T11:11:43.452978Z","shell.execute_reply":"2024-04-19T11:11:43.458246Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"This is how 1 sequence looks.\nOutput sequence is (input+1) at each index.\nThis creates a target for model to predict since at each position, It shows the next predicted item at each position.\nIt can be seen as training examples equal to block_size because you can take any sequence at index (0-n) and its target would be at index (n) in output sequence.","metadata":{}},{"cell_type":"code","source":"x,y = get_batch('train')\nx = x[0]\ny = y[0]\nprint(x)\nprint(y)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B-5-usEHSzm3","outputId":"2a115d8f-4c3e-4a01-a5ef-0ba0fa53dcb0","execution":{"iopub.status.busy":"2024-04-19T11:11:43.460303Z","iopub.execute_input":"2024-04-19T11:11:43.460593Z","iopub.status.idle":"2024-04-19T11:11:43.672569Z","shell.execute_reply.started":"2024-04-19T11:11:43.460571Z","shell.execute_reply":"2024-04-19T11:11:43.671684Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"tensor([3076, 2747,  539, 3189, 2738, 2407, 3129, 2245, 2779,  367, 1220, 2539,\n        3614, 1958, 3052, 2469, 2870, 2671, 3263, 3501, 2109, 3448, 1083, 1409,\n        2918, 3755, 3863, 2404, 1270, 1210,    1, 1267,  541, 1233, 1610,  589,\n         110,  457, 2028,  480, 1356, 1480,  377, 2916, 2628,  733, 1573,  555,\n        1370,  707, 1617, 3471, 1214, 3703,  260, 1196,  750, 1206, 1240, 2529,\n        1199, 1374, 1097,   32, 2009, 1584, 1653, 2571,   21, 1372, 2353,  608,\n         318, 2858,  593,  213, 1266,  590, 1208,  912, 1734,  598,  257,  563,\n        1280, 1446, 3142, 1288, 1220, 1381, 1380, 2877, 1253, 1252,  444,  671,\n        2498, 1860,  214,  913, 3334, 2951, 1304, 2922, 1201, 3681, 2943, 3745,\n         904,  924,   50, 2762, 3784, 1175,   29, 2997, 3793, 2324, 3948, 2594,\n        1580, 3175, 3863, 3555, 3300, 2605,  908, 1196, 2136, 2353, 1961, 1198,\n        2763, 3911, 3897, 3948, 2858, 3481, 3160,  110, 2791, 1259, 3555, 2291,\n        3624, 1580,  480,   70, 1544, 3553, 3916, 3751, 3408, 3753, 3623, 3326,\n        3824, 3579, 3536, 3578, 1393,  318,  296, 1242,  608, 1231, 1704, 2248,\n         920, 3424, 2959, 1968, 1584,  150, 3138,  590, 2268, 3418,  493, 1466,\n        1678, 1729, 1653, 2000, 1784,  161, 1268,  322, 1672, 1183, 1059, 1476,\n        1721, 3448, 2240, 1589,  508,  627, 2272, 2712, 3155, 3176,  292, 1711,\n        2271, 1586, 1093, 3053, 1363, 2374,  465,   74, 2297, 1726, 1302, 3755,\n        3316, 2890, 2058, 3452,   50, 2028, 1617, 1197, 2997,  260, 2396, 1214,\n        2237, 3793, 2291, 2724, 1693, 1617, 1909, 3910, 3911, 3880, 1269,  908,\n        3481, 2858, 1673, 3289,  110,  661,  551,  144, 3512, 3753, 3160, 2396,\n        2692, 1094, 1265,  247], device='cuda:0')\ntensor([2747,  539, 3189, 2738, 2407, 3129, 2245, 2779,  367, 1220, 2539, 3614,\n        1958, 3052, 2469, 2870, 2671, 3263, 3501, 2109, 3448, 1083, 1409, 2918,\n        3755, 3863, 2404, 1270, 1210,    1, 1267,  541, 1233, 1610,  589,  110,\n         457, 2028,  480, 1356, 1480,  377, 2916, 2628,  733, 1573,  555, 1370,\n         707, 1617, 3471, 1214, 3703,  260, 1196,  750, 1206, 1240, 2529, 1199,\n        1374, 1097,   32, 2009, 1584, 1653, 2571,   21, 1372, 2353,  608,  318,\n        2858,  593,  213, 1266,  590, 1208,  912, 1734,  598,  257,  563, 1280,\n        1446, 3142, 1288, 1220, 1381, 1380, 2877, 1253, 1252,  444,  671, 2498,\n        1860,  214,  913, 3334, 2951, 1304, 2922, 1201, 3681, 2943, 3745,  904,\n         924,   50, 2762, 3784, 1175,   29, 2997, 3793, 2324, 3948, 2594, 1580,\n        3175, 3863, 3555, 3300, 2605,  908, 1196, 2136, 2353, 1961, 1198, 2763,\n        3911, 3897, 3948, 2858, 3481, 3160,  110, 2791, 1259, 3555, 2291, 3624,\n        1580,  480,   70, 1544, 3553, 3916, 3751, 3408, 3753, 3623, 3326, 3824,\n        3579, 3536, 3578, 1393,  318,  296, 1242,  608, 1231, 1704, 2248,  920,\n        3424, 2959, 1968, 1584,  150, 3138,  590, 2268, 3418,  493, 1466, 1678,\n        1729, 1653, 2000, 1784,  161, 1268,  322, 1672, 1183, 1059, 1476, 1721,\n        3448, 2240, 1589,  508,  627, 2272, 2712, 3155, 3176,  292, 1711, 2271,\n        1586, 1093, 3053, 1363, 2374,  465,   74, 2297, 1726, 1302, 3755, 3316,\n        2890, 2058, 3452,   50, 2028, 1617, 1197, 2997,  260, 2396, 1214, 2237,\n        3793, 2291, 2724, 1693, 1617, 1909, 3910, 3911, 3880, 1269,  908, 3481,\n        2858, 1673, 3289,  110,  661,  551,  144, 3512, 3753, 3160, 2396, 2692,\n        1094, 1265,  247, 1188], device='cuda:0')\n","output_type":"stream"}]},{"cell_type":"markdown","source":"This function calculates loss and accuracy after specific number of epochs for validation.\nIt takes mean of loss of multiple test examples to calculate average loss.","metadata":{}},{"cell_type":"code","source":"@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    out1 = {}\n    model.eval()\n    for split in ['train', 'val']:\n        accuracies = torch.zeros(eval_iters)\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            loss,acc = model(X, Y)\n            losses[k] = loss.item()\n            accuracies[k] = acc.item()\n        out[split] = losses.mean()\n        out1[split] = accuracies.mean()\n    model.train()\n    return out,out1","metadata":{"id":"5s8ytjo1aYyA","execution":{"iopub.status.busy":"2024-04-19T11:11:43.674165Z","iopub.execute_input":"2024-04-19T11:11:43.674452Z","iopub.status.idle":"2024-04-19T11:11:43.681536Z","shell.execute_reply.started":"2024-04-19T11:11:43.674429Z","shell.execute_reply":"2024-04-19T11:11:43.680577Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"This is the model\nModel Diagram is given in Repository","metadata":{}},{"cell_type":"code","source":"\nclass GPT2(nn.Module):\n    def __init__(self, vocab_size, n_embd=768, n_head=12, n_layer=12, block_size=512):\n        super(GPT2, self).__init__()\n        self.vocab_size = vocab_size\n        self.n_embd = n_embd\n        self.n_head = n_head\n        self.n_layer = n_layer\n        self.block_size = block_size\n\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.positional_embedding_table = nn.Embedding(block_size, n_embd)\n        self.layers = nn.ModuleList([TransformerBlock(n_embd, n_head) for _ in range(n_layer)])\n        self.ln_f = nn.LayerNorm(n_embd)\n        self.ln_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, input_ids, targets=None):\n        token_embeddings = self.token_embedding_table(input_ids)\n        position_ids = torch.arange(input_ids.size(1), dtype=torch.long, device=input_ids.device)\n        position_embeddings = self.positional_embedding_table(position_ids)\n\n        hidden_states = token_embeddings + position_embeddings\n\n        for layer in self.layers:\n            hidden_states = layer(hidden_states)\n\n        logits = self.ln_head(hidden_states)\n\n        if targets is not None:\n            loss = F.cross_entropy(logits.view(-1, self.vocab_size), targets.view(-1))\n            acc = self.top_k_accuracy(logits, targets, k=10)\n            return loss, acc\n        else:\n            return logits\n\n    def generate(self, input_ids, max_len=20):\n        for _ in range(max_len):\n            logits = self.forward(input_ids)\n            next_token = torch.argmax(logits[:, -1, :], dim=-1).unsqueeze(-1)\n            input_ids = torch.cat([input_ids, next_token], dim=-1)\n        return input_ids\n\n    def top_k_accuracy(self, logits, targets, k=10):\n        _, indices = logits.topk(k, dim=-1)\n        correct = torch.eq(indices, targets.unsqueeze(-1))\n        correct_k = correct.any(dim=-1).float()\n        return correct_k.mean()\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, n_embd, n_head):\n        super(TransformerBlock, self).__init__()\n        self.attention = MultiHeadAttention(n_embd, n_head)\n        self.feed_forward = FeedForward(n_embd)\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n\n    def forward(self, x):\n        x = x + self.attention(self.ln1(x))\n        x = x + self.feed_forward(self.ln2(x))\n        return x\n\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, n_embd, n_head):\n        super(MultiHeadAttention, self).__init__()\n        self.heads = nn.ModuleList([SingleHeadAttention(n_embd) for _ in range(n_head)])\n        self.proj = nn.Linear(n_embd * n_head, n_embd)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        out = torch.cat([head(x) for head in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out\n\n\nclass SingleHeadAttention(nn.Module):\n    def __init__(self, n_embd):\n        super(SingleHeadAttention, self).__init__()\n        self.key = nn.Linear(n_embd, n_embd)\n        self.query = nn.Linear(n_embd, n_embd)\n        self.value = nn.Linear(n_embd, n_embd)\n        self.register_buffer('tril',torch.tril(torch.ones(block_size,block_size)))\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        B,T,C = x.shape\n        key = self.key(x)\n        query = self.query(x)\n        value = self.value(x)\n        attention_weights = torch.matmul(query, key.transpose(-2, -1)) / torch.sqrt(torch.tensor(x.size(-1)).float())\n        attention_weights = attention_weights.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n        attention_weights = F.softmax(attention_weights, dim=-1)\n        attention_weights = self.dropout(attention_weights)\n        out = torch.matmul(attention_weights, value)\n        return out\n\n\nclass FeedForward(nn.Module):\n    def __init__(self, n_embd):\n        super(FeedForward, self).__init__()\n        self.linear1 = nn.Linear(n_embd, 4 * n_embd)\n        self.activation = nn.ReLU()\n        self.linear2 = nn.Linear(4 * n_embd, n_embd)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        x = self.linear1(x)\n        x = self.activation(x)\n        x = self.linear2(x)\n        x = self.dropout(x)\n        return x\n\n\n","metadata":{"id":"hQdY1Buggbs2","execution":{"iopub.status.busy":"2024-04-19T11:11:43.683041Z","iopub.execute_input":"2024-04-19T11:11:43.683312Z","iopub.status.idle":"2024-04-19T11:11:43.718654Z","shell.execute_reply.started":"2024-04-19T11:11:43.683290Z","shell.execute_reply":"2024-04-19T11:11:43.717739Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"Here we create the model object and train it.\nAcc metric is Hit@10 metric which checks whether the targer item is available in the top 10 probabilities predicted by model.","metadata":{}},{"cell_type":"code","source":"\n\nmodel = GPT2(vocab_size=vocab_size,n_embd=n_embd, n_head=n_head, n_layer=n_layer, block_size=block_size)\nm = model.to(device)\n\n# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n\n    # every once in a while evaluate the loss on train and val sets\n    if iter%100 == 0:\n      print(iter)\n\n    if iter % eval_interval == 0:\n        losses,acc = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n        print(f\"step {iter}: train acc {acc['train']:.4f}, val acc {acc['val']:.4f}\")\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    loss,acc = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"Ke0IZPKOjz9Q","outputId":"1c03308d-782a-4f12-956e-ff5d2cd36c65","execution":{"iopub.status.busy":"2024-04-19T11:11:43.719743Z","iopub.execute_input":"2024-04-19T11:11:43.720079Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"0\nstep 0: train loss 8.6790, val loss 8.6808\nstep 0: train acc 0.0025, val acc 0.0026\n100\nstep 100: train loss 7.3587, val loss 7.4054\nstep 100: train acc 0.0573, val acc 0.0516\n200\nstep 200: train loss 6.4473, val loss 6.5857\nstep 200: train acc 0.1391, val acc 0.1189\n300\nstep 300: train loss 6.1018, val loss 6.2869\nstep 300: train acc 0.1751, val acc 0.1460\n400\nstep 400: train loss 5.8990, val loss 6.1356\nstep 400: train acc 0.1992, val acc 0.1622\n500\nstep 500: train loss 5.7594, val loss 6.0575\nstep 500: train acc 0.2166, val acc 0.1699\n600\nstep 600: train loss 5.6561, val loss 5.9679\nstep 600: train acc 0.2298, val acc 0.1799\n700\nstep 700: train loss 5.5684, val loss 5.9250\nstep 700: train acc 0.2438, val acc 0.1850\n800\nstep 800: train loss 5.4794, val loss 5.8876\nstep 800: train acc 0.2581, val acc 0.1914\n900\nstep 900: train loss 5.4180, val loss 5.8636\nstep 900: train acc 0.2680, val acc 0.1952\n1000\nstep 1000: train loss 5.3675, val loss 5.8295\nstep 1000: train acc 0.2766, val acc 0.1998\n1100\nstep 1100: train loss 5.3035, val loss 5.8074\nstep 1100: train acc 0.2868, val acc 0.2016\n1200\nstep 1200: train loss 5.2578, val loss 5.7902\nstep 1200: train acc 0.2947, val acc 0.2066\n1300\nstep 1300: train loss 5.2049, val loss 5.7947\nstep 1300: train acc 0.3041, val acc 0.2083\n1400\nstep 1400: train loss 5.1394, val loss 5.7793\nstep 1400: train acc 0.3163, val acc 0.2109\n1500\nstep 1500: train loss 5.1079, val loss 5.7736\nstep 1500: train acc 0.3223, val acc 0.2117\n1600\nstep 1600: train loss 5.0799, val loss 5.7713\nstep 1600: train acc 0.3280, val acc 0.2131\n1700\nstep 1700: train loss 5.0241, val loss 5.7559\nstep 1700: train acc 0.3374, val acc 0.2129\n1800\nstep 1800: train loss 4.9956, val loss 5.7520\nstep 1800: train acc 0.3437, val acc 0.2160\n1900\nstep 1900: train loss 4.9455, val loss 5.7544\nstep 1900: train acc 0.3536, val acc 0.2178\n2000\nstep 2000: train loss 4.9014, val loss 5.7695\nstep 2000: train acc 0.3619, val acc 0.2190\n2100\nstep 2100: train loss 4.8647, val loss 5.7857\nstep 2100: train acc 0.3706, val acc 0.2154\n2200\nstep 2200: train loss 4.8148, val loss 5.7626\nstep 2200: train acc 0.3780, val acc 0.2209\n2300\nstep 2300: train loss 4.7692, val loss 5.7797\nstep 2300: train acc 0.3872, val acc 0.2194\n2400\nstep 2400: train loss 4.7271, val loss 5.7841\nstep 2400: train acc 0.3962, val acc 0.2189\n2500\nstep 2500: train loss 4.6842, val loss 5.7869\nstep 2500: train acc 0.4057, val acc 0.2185\n2600\nstep 2600: train loss 4.6562, val loss 5.8159\nstep 2600: train acc 0.4121, val acc 0.2148\n2700\nstep 2700: train loss 4.6221, val loss 5.8141\nstep 2700: train acc 0.4182, val acc 0.2160\n2800\nstep 2800: train loss 4.5748, val loss 5.8158\nstep 2800: train acc 0.4287, val acc 0.2185\n2900\nstep 2900: train loss 4.5305, val loss 5.8314\nstep 2900: train acc 0.4372, val acc 0.2191\n3000\nstep 3000: train loss 4.4954, val loss 5.8388\nstep 3000: train acc 0.4447, val acc 0.2188\n3100\nstep 3100: train loss 4.4649, val loss 5.8473\nstep 3100: train acc 0.4510, val acc 0.2176\n3200\nstep 3200: train loss 4.4238, val loss 5.8652\nstep 3200: train acc 0.4604, val acc 0.2184\n3300\nstep 3300: train loss 4.3755, val loss 5.8848\nstep 3300: train acc 0.4689, val acc 0.2174\n","output_type":"stream"}]},{"cell_type":"markdown","source":"This is how this model can be used to predict next items after training.","metadata":{}},{"cell_type":"code","source":"for i in range(5):\n    print(i)\n    xb, yb = get_batch('train')\n    x = xb[0]\n    y = yb[0]\n    x = x.unsqueeze(0)\n    y = y.unsqueeze(0)\n    logits = model(x)\n    predicted_sequence = torch.argmax(logits, dim=-1)  # Shape: (1, T)\n    print(y)\n    print(predicted_sequence)","metadata":{"id":"mLK881Rf7SnT","trusted":true},"execution_count":null,"outputs":[]}]}